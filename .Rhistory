emw <- read_dta("data/efmw_replication.dta")
# create two variables, following the authors' replication code
emw$dist <- log(1 + (1 / (emw$dist_coast)))
emw$distwremit <- log(1 + ( (emw$richremit / 1000000) * (emw$dist)))
# limit the data to relevant columns and complete cases of those variables
emw <- emw[, c("Protest", "remit", "dict", "l1gdp", "l1pop", "l1nbr5", "l12gr",
"l1migr", "elec3", "cowcode", "period", "distwremit",
"caseid", "year")]
emw <- na.omit(emw)
controls <- c("l1gdp", "l1pop", "l1nbr5", "l12gr", "l1migr", "elec3")
# Generate fixed effects (don't worry too much about this)
contr.list <- list(contr.sum, contr.sum)
names(contr.list) <- c("factor(period)","factor(cowcode)")
mod_mat <- model.matrix(~factor(period)+factor(cowcode),data=emw,contrasts.arg=contr.list)[,-1]
# Matrix of controls incl. fixed effects
X <- as.matrix(cbind(emw[,controls],mod_mat))
# Moderator of interest
V <- emw$dict
# Interacted version
VX <- as.matrix(V*X)
# Treatment
D <- emw$remit
# Interacted treatment
DV <- D*V
# Outcome
Y <- emw$Protest
#### Stage 1. Estimate LASSO models ####
lasso_selector <- function(LHS, RHS, folds) {
# Merge data on the right hand side into matrix
RHS_matrix <- do.call("cbind", RHS)
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS_matrix, y = LHS, alpha = 1, nfolds = folds)$lambda.min
# Estimate final model
lasso <- glmnet(x=RHS_matrix,y=LHS, lambda = cv_lambda)
# Find non-zero coefficients
coef_index <- which(coef(lasso) != 0) - 1 # Minus 1 to remove the intercept term
coef_names <- colnames(RHS_matrix)[coef_index]
return(coef_names)
}
## y on {v,X,vX}
Y_lasso <-  lasso_selector(LHS = Y, RHS = list(V,X,VX), folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = list(V,X,VX), folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = list(V,X,VX), folds = 10)
selected_columns <- union(Y_lasso, D_lasso, DV_lasso)
selected_columns <- union(Y_lasso, D_lasso)
selected_columns <- union(selected_columns, DV_lasso)
Y_lasso <-  lasso_selector(LHS = Y, RHS = list(V,X,VX), folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = list(V,X,VX), folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = list(V,X,VX), folds = 10)
selected_columns <- union(Y_lasso, D_lasso)
selected_columns <- union(selected_columns, DV_lasso)
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 2/5                                 ##
##                                                                            ##
##                          LASSO Models and Cross Validation                 ##
##                            Walkthrough of extensions                       ##
##                                                                            ##
################################################################################
## NB: This code adapts the replication materials contained in Blackwell and
## Olson (2021), ``Reducing Model Misspecification and Bias in the Estimation of
## Interactions'', Available at: https://doi.org/10.7910/DVN/HZYFRI
# install.packages("haven")
# install.packages("lfe")
# install.packages("glmnet")
library(haven)
library(lfe)
library(glmnet)
emw <- read_dta("data/efmw_replication.dta")
# create two variables, following the authors' replication code
emw$dist <- log(1 + (1 / (emw$dist_coast)))
emw$distwremit <- log(1 + ( (emw$richremit / 1000000) * (emw$dist)))
# limit the data to relevant columns and complete cases of those variables
emw <- emw[, c("Protest", "remit", "dict", "l1gdp", "l1pop", "l1nbr5", "l12gr",
"l1migr", "elec3", "cowcode", "period", "distwremit",
"caseid", "year")]
emw <- na.omit(emw)
controls <- c("l1gdp", "l1pop", "l1nbr5", "l12gr", "l1migr", "elec3")
# Generate fixed effects (don't worry too much about this)
contr.list <- list(contr.sum, contr.sum)
names(contr.list) <- c("factor(period)","factor(cowcode)")
mod_mat <- model.matrix(~factor(period)+factor(cowcode),data=emw,contrasts.arg=contr.list)[,-1]
# Matrix of controls incl. fixed effects
X <- as.matrix(cbind(emw[,controls],mod_mat))
# Moderator of interest
V <- emw$dict
# Interacted version
VX <- as.matrix(V*X)
# Treatment
D <- emw$remit
# Interacted treatment
DV <- D*V
# Outcome
Y <- emw$Protest
#### Stage 1. Estimate LASSO models ####
lasso_selector <- function(LHS, RHS, folds) {
# Merge data on the right hand side into matrix
RHS_matrix <- do.call("cbind", RHS)
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS_matrix, y = LHS, alpha = 1, nfolds = folds)$lambda.min
# Estimate final model
lasso <- glmnet(x=RHS_matrix,y=LHS, lambda = cv_lambda)
# Find non-zero coefficients
coef_index <- which(coef(lasso) != 0) - 1 # Minus 1 to remove the intercept term
coef_names <- colnames(RHS_matrix)[coef_index]
return(coef_names)
}
## y on {v,X,vX}
Y_lasso <-  lasso_selector(LHS = Y, RHS = list(V,X,VX), folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = list(V,X,VX), folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = list(V,X,VX), folds = 10)
selected_columns <- union(Y_lasso, D_lasso)
selected_columns <- union(selected_columns, DV_lasso)
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
Y_lasso <-  lasso_selector(LHS = Y, RHS = list(V,X,VX), folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = list(V,X,VX), folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = list(V,X,VX), folds = 10)
c(Y_lasso, D_lasso, DV_lasso)
unique(c(Y_lasso, D_lasso, DV_lasso))
Y_lasso
LHS = Y
Y
RHS = list(V,X,VX)
folds = 10
# Merge data on the right hand side into matrix
RHS_matrix <- do.call("cbind", RHS)
dims(RHS_matrix)
dim(RHS_matrix)
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS_matrix, y = LHS, alpha = 1, nfolds = folds)$lambda.min
cv_lambda
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS_matrix, y = LHS, alpha = 1, nfolds = folds)$lambda.min
cv_lambda
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS_matrix, y = LHS, alpha = 1, nfolds = folds)$lambda.min
cv_lambda
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS_matrix, y = LHS, alpha = 1, nfolds = folds)$lambda.min
cv_lambda
# Estimate final model
lasso <- glmnet(x=RHS_matrix,y=LHS, lambda = cv_lambda)
which(coef(lasso) != 0)
# Find non-zero coefficients
coef_index <- which(coef(lasso) != 0) - 1 # Minus 1 to remove the intercept term
coef_index
coef_names <- colnames(RHS_matrix)[coef_index]
colnames(RHS_matrix)
coef_index
colnames(RHS_matrix)[coef_index]
colnames(RHS_matrix)
View(RHS_matrix)
View(RHS_matrix)
RHS
# Interacted treatment
DV <- as.matrix(D*V)
View(DV)
DV <- as.matrix(D*V)
View(VX)
RHS_matrix <- as.matrix(cbind(V,X,VX))
View(RHS_matrix)
# Optional but useful
colnames(RHS_matrix) <- c(colnames(V), colnames(X),
paste0("V_",colnames(X)))
colnames(V)
V
colnames(X)
c("V", colnames(X),
paste0("V_",colnames(X)))
c("V", colnames(X),
paste0("V_",colnames(X)))
RHS_matrix <- as.matrix(cbind(V,X,VX))
# Optional but useful to keep track of names
colnames(RHS_matrix) <- c("V", colnames(X),
paste0("V_",colnames(X)))
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, folds = 10)
lasso_selector <- function(LHS, RHS, folds) {
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS, y = LHS, alpha = 1, nfolds = folds)$lambda.min
# Estimate final model
lasso <- glmnet(x=RHS_matrix,y=LHS, lambda = cv_lambda)
# Find non-zero coefficients
coef_index <- which(coef(lasso) != 0) - 1
return(coef_index)
}
RHS_matrix <- as.matrix(cbind(V,X,VX))
# Optional but useful to keep track of names
colnames(RHS_matrix) <- c("V", colnames(X),
paste0("V_",colnames(X)))
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = RHS_matrix, folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix, folds = 10)
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
RHS_matrix(,c(0,1,2))
RHS_matrix
RHS_matrix[,c(0,1,2)]
RHS_matrix[,c(1,2)]
ols_matrix <- as.matrix(cbind(Y,D,DV,RHS_matrix[,selected_columns]))
ols_model <- lm("Y~.", data = ols_matrix)
ols_matrix <- cbind(Y,D,DV,RHS_matrix[,selected_columns])
ols_model <- lm("Y~.", data = ols_matrix)
ols_model <- glm("Y~.", data = ols_matrix)
ols_matrix <- as.data.frame(cbind(Y,D,DV,RHS_matrix[,selected_columns]))
ols_model <- glm("Y~.", data = ols_matrix)
ols_model
summary(ols_model)
View(VX)
View(RHS)
View(RHS_matrix)
names(mod_mat)
colnames(mod_mat)
glm(paste0("Y ~ D + V + DV",controls,colnames(mod_mat), sep = " + "))
paste0("Y ~ D + V + DV",controls,colnames(mod_mat), sep = " + ")
paste0(c("Y ~ D + V + DV",controls,colnames(mod_mat)), sep = " + ")
paste0(c("Y ~ D + V + DV",controls,colnames(mod_mat)), collapse = " + ")
paste0(c("Y ~ D + V + DV",controls,colnames(mod_mat)), collapse = " + ")
Y
glm(paste0(c("Y ~ D + V + DV",controls,colnames(mod_mat)), collapse = " + "),
data = RHS_matrix)
paste0(c("Y ~ D + V + DV",controls,colnames(mod_mat)), collapse = " + ")
contr.sum
c("Y ~ D + V + DV",controls,"period + cowcode"), collapse = " + ")
paste0(c("Y ~ D + V + DV",controls,"period + cowcode"), collapse = " + ")
glm(paste0(c("Y ~ D + V + DV",controls,"period + cowcode"), collapse = " + "),
data = RHS_matrix)
glm(paste0(c("Y ~ D + V + DV",controls,"period + cowcode"), collapse = " + "),
data = ols_matrix)
emw$Protest
glm(paste0(c("Protest ~ D + remit*dict",controls,"period + cowcode"), collapse = " + "),
data = ols_matrix)
glm(paste0(c("Protest ~ D + remit*dict",controls,"period + cowcode"), collapse = " + "),
data = emw)
glm(paste0(c("Protest ~ remit*dict",controls,"period + cowcode"), collapse = " + "),
data = emw)
glm(paste0(c("Protest ~ remit*dict",controls,"as.factor(period) + as.factor(cowcode)"), collapse = " + "),
data = emw)
naive_model <- glm(paste0(c("Protest ~ remit*dict",controls,"as.factor(period) + as.factor(cowcode)"), collapse = " + "),
data = emw)
summary(naive_model)
ds_model <- glm("Y~.", data = ols_matrix)
naive_model <- glm(paste0(c("Protest ~ remit*dict",controls,"as.factor(period) + as.factor(cowcode)"), collapse = " + "),
data = emw)
summary(ds_model)
RHS_matrix <- as.matrix(cbind(V = V,X,VX))
RHS_matrix
colnames(RHS_matrix)
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = RHS_matrix, folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix, folds = 10)
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
dim(RHS_matrix)
ols_matrix <- as.data.frame(cbind(Y=Y,D=D,DV=DV,RHS_matrix[,selected_columns]))
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 2/5                                 ##
##                                                                            ##
##                          LASSO Models and Cross Validation                 ##
##                            Walkthrough of extensions                       ##
##                                                                            ##
################################################################################
## NB: This code adapts the replication materials contained in Blackwell and
## Olson (2021), ``Reducing Model Misspecification and Bias in the Estimation of
## Interactions'', Available at: https://doi.org/10.7910/DVN/HZYFRI
# install.packages("haven")
# install.packages("lfe")
# install.packages("glmnet")
library(haven)
library(lfe)
library(glmnet)
set.seed(89)
emw <- read_dta("data/efmw_replication.dta")
# create two variables, following the authors' replication code
emw$dist <- log(1 + (1 / (emw$dist_coast)))
emw$distwremit <- log(1 + ( (emw$richremit / 1000000) * (emw$dist)))
# limit the data to relevant columns and complete cases of those variables
emw <- emw[, c("Protest", "remit", "dict", "l1gdp", "l1pop", "l1nbr5", "l12gr",
"l1migr", "elec3", "cowcode", "period", "distwremit",
"caseid", "year")]
emw <- na.omit(emw)
controls <- c("l1gdp", "l1pop", "l1nbr5", "l12gr", "l1migr", "elec3")
# Generate fixed effects (don't worry too much about this)
contr.list <- list(contr.sum, contr.sum)
names(contr.list) <- c("factor(period)","factor(cowcode)")
mod_mat <- model.matrix(~factor(period)+factor(cowcode),data=emw,contrasts.arg=contr.list)[,-1]
# Matrix of controls incl. fixed effects
X <- as.matrix(cbind(emw[,controls],mod_mat))
# Moderator of interest
V <- emw$dict
# Interacted version
VX <- as.matrix(V*X)
# Treatment
D <- emw$remit
# Interacted treatment
DV <- D*V
# Outcome
Y <- emw$Protest
#### Stage 1. Estimate LASSO models ####
lasso_selector <- function(LHS, RHS, folds) {
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS, y = LHS, alpha = 1, nfolds = folds)$lambda.min
# Estimate final model
lasso <- glmnet(x=RHS_matrix,y=LHS, lambda = cv_lambda)
# Find non-zero coefficients
coef_index <- which(coef(lasso) != 0) - 1
return(coef_index)
}
## Define RHS matrix
RHS_matrix <- as.matrix(cbind(V = V,X,VX))
# Optional but useful to keep track of names
colnames(RHS_matrix) <- c("V", colnames(X),
paste0("V_",colnames(X)))
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = RHS_matrix, folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix, folds = 10)
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
#### Stage 2. Estimate inference model
ols_matrix <- as.data.frame(cbind(Y=Y,D=D,DV=DV,RHS_matrix[,selected_columns]))
View(ols_matrix)
ds_model <- glm("Y~.", data = ols_matrix)
naive_model <- glm(paste0(c("Protest ~ remit*dict",controls,"as.factor(period) + as.factor(cowcode)"), collapse = " + "),
data = emw)
summary(ds_model)
summary(naive_model)
ols_matrix <- as.data.frame(cbind(Protest=Y,remit=D,`remit:dict`=DV,RHS_matrix[,selected_columns]))
ols_matrix <- as.data.frame(cbind(Protest=Y,remit=D,remit_dict=DV,RHS_matrix[,selected_columns]))
ds_model <- glm("Y~.", data = ols_matrix)
naive_model <- glm(paste0(c("Protest ~ remit*dict",controls,"as.factor(period) + as.factor(cowcode)"), collapse = " + "),
data = emw)
summary(ds_model)
summary(naive_model)
summary(ds_model)
test <- summary(ds_model)
View(test)
test <- summary(ds_model)[,"remit_dict"]
?glmnet
library(haven)
library(lfe)
library(glmnet)
set.seed(89)
emw <- read_dta("data/efmw_replication.dta")
emw$dist <- log(1 + (1 / (emw$dist_coast)))
emw$distwremit <- log(1 + ( (emw$richremit / 1000000) * (emw$dist)))
emw <- emw[, c("Protest", "remit", "dict", "l1gdp", "l1pop", "l1nbr5", "l12gr",
"l1migr", "elec3", "cowcode", "period", "distwremit",
"caseid", "year")]
emw <- na.omit(emw)
controls <- c("l1gdp", "l1pop", "l1nbr5", "l12gr", "l1migr", "elec3")
# Generate fixed effects (don't worry too much about this)
contr.list <- list(contr.sum, contr.sum)
names(contr.list) <- c("factor(period)","factor(cowcode)")
mod_mat <- model.matrix(~factor(period)+factor(cowcode),data=emw,contrasts.arg=contr.list)[,-1]
# Matrix of controls incl. fixed effects
X <- as.matrix(cbind(emw[,controls],mod_mat))
# Moderator of interest
V <- emw$dict
# Interacted version
VX <- as.matrix(V*X)
# Treatment
D <- emw$remit
# Interacted treatment
DV <- D*V
# Outcome
Y <- emw$Protest
lasso_selector <- function(LHS, RHS, folds) {
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS, y = LHS, alpha = 1, nfolds = folds)$lambda.min
# Estimate final model
lasso <- glmnet(x=RHS_matrix,y=LHS, lambda = cv_lambda)
# Find non-zero coefficients
coef_index <- which(coef(lasso) != 0) - 1
return(coef_index)
}
RHS_matrix <- as.matrix(cbind(V = V,X,VX))
# Optional but useful to keep track of names
colnames(RHS_matrix) <- c("V", colnames(X),
paste0("V_",colnames(X)))
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = RHS_matrix, folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix, folds = 10)
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
ols_matrix <- as.data.frame(cbind(Protest=Y,remit=D,remit_dict=DV,RHS_matrix[,selected_columns]))
ds_model <- glm("Y~.", data = ols_matrix)
naive_model <- glm(paste0(c("Protest ~ remit*dict",controls,"as.factor(period) + as.factor(cowcode)"), collapse = " + "),
data = emw)
summary(ds_model)
summary(naive_model)
fmod_model <- glm("Y~.",
data = as.data.frame(cbind(Protest=Y,remit=D,remit_dict=DV,RHS_matrix)))
summary(fmod_model)
################################################################################
##                                                                            ##
##                              Oxford Spring School                          ##
##                                Machine Learning                            ##
##                                    Day 2/5                                 ##
##                                                                            ##
##                          LASSO Models and Cross Validation                 ##
##                            Walkthrough of extensions                       ##
##                                                                            ##
################################################################################
## NB: This code adapts the replication materials contained in Blackwell and
# Olson (2021), ``Reducing Model Misspecification and Bias in the Estimation of
# Interactions'', available at: https://doi.org/10.7910/DVN/HZYFRI
## NB: For the purpose of demonstration, we'll make several simplifying
# assumptions about the post-double selection procedure and the inference model
# install.packages("haven")
# install.packages("lfe")
# install.packages("glmnet")
library(haven)
library(lfe)
library(glmnet)
set.seed(89)
emw <- read_dta("data/efmw_replication.dta")
# create two variables, following the authors' replication code
emw$dist <- log(1 + (1 / (emw$dist_coast)))
emw$distwremit <- log(1 + ( (emw$richremit / 1000000) * (emw$dist)))
# limit the data to relevant columns and complete cases of those variables
emw <- emw[, c("Protest", "remit", "dict", "l1gdp", "l1pop", "l1nbr5", "l12gr",
"l1migr", "elec3", "cowcode", "period", "distwremit",
"caseid", "year")]
emw <- na.omit(emw)
controls <- c("l1gdp", "l1pop", "l1nbr5", "l12gr", "l1migr", "elec3")
# Generate fixed effects (don't worry too much about this)
contr.list <- list(contr.sum, contr.sum)
names(contr.list) <- c("factor(period)","factor(cowcode)")
mod_mat <- model.matrix(~factor(period)+factor(cowcode),data=emw,contrasts.arg=contr.list)[,-1]
# Matrix of controls incl. fixed effects
X <- as.matrix(cbind(emw[,controls],mod_mat))
# Moderator of interest
V <- emw$dict
# Interacted version
VX <- as.matrix(V*X)
# Treatment
D <- emw$remit
# Interacted treatment
DV <- D*V
# Outcome
Y <- emw$Protest
#### Stage 1. Estimate LASSO models ####
lasso_selector <- function(LHS, RHS, folds) {
# Get best lambda using cross-fold validation
cv_lambda <- cv.glmnet(x = RHS, y = LHS, alpha = 1, nfolds = folds)$lambda.min
# Estimate final model
lasso <- glmnet(x=RHS_matrix,y=LHS, lambda = cv_lambda)
# Find non-zero coefficients
coef_index <- which(coef(lasso) != 0) - 1
return(coef_index)
}
## Define RHS matrix
RHS_matrix <- as.matrix(cbind(V = V,X,VX))
# Optional but useful to keep track of names
colnames(RHS_matrix) <- c("V", colnames(X),
paste0("V_",colnames(X)))
Y_lasso <-  lasso_selector(LHS = Y, RHS = RHS_matrix, folds = 10)
D_lasso <-  lasso_selector(LHS = D, RHS = RHS_matrix, folds = 10)
DV_lasso <- lasso_selector(LHS = DV, RHS = RHS_matrix, folds = 10)
selected_columns <- unique(c(Y_lasso, D_lasso, DV_lasso))
#### Stage 2. Estimate inference model
ds_matrix <- as.data.frame(cbind(Protest=Y,
remit=D,
remit_dict=DV,
RHS_matrix[,selected_columns]))
ds_model <- glm("Y~.", data = ds_matrix)
naive_model <- glm(paste0(c("Protest ~ remit*dict",
controls,
"as.factor(period) + as.factor(cowcode)"),
collapse = " + "),
data = emw)
fmod_model <- glm("Y~.",
data = as.data.frame(cbind(Protest=Y,
remit=D,
remit_dict=DV,
RHS_matrix)))
summary(ds_model)
summary(naive_model)
summary(fmod_model)
#### Extension task 1 ####
# 1. Generate a training dataset X with 100 variables and 2000 observations,
# where each observation is a draw from a random uniform distribution between -5
# and 5.
# 2. Generate an outcome vector Y that has the following features
#   a. Linear in relation to all 100 variables
#   b. As X1 increases by 1, Y increases by 10
#   c. As X2 increases by 1, Y decreases by 5
#   d. X3-X10 do not affect Y
#   e. X11-X100 have coefficients drawn from a random normal distribution with
# mean = 0, sd = 0.05
# 3. Estimate a cross-validated LASSO model to find lambda
# 4. Estimate a final LASSO model using the results from 3.
# Qs. What are the sizes of coefficient X1 X2? Do X3-X10 have non-zero
# coefficients? What about X11-X100?
#### Extension task 2 ####
## Blackwell and Olson propose a slightly more complicated regularization
# procedure in the post-double selection, where they apply separate
# regularisation to each coefficient.
# Further information can be found on pages 14-15 of the article, available here:
# https://mattblackwell.org/files/papers/lasso-inters.pdf
## To see this in action, look at the rlasso_cluster function from line 221 in
# the following file:
# https://github.com/mattblackwell/inters/blob/master/R/lasso_interactions.R
# 1. Using the help file for glmnet, i.e. type `?glmnet` into the console, and
# looking at lines 252-254 in the github file above, what argument is passed to
# glmnet to penalize individual coefficient values?
# 2. Without worrying too much about the surrounding code, describe what
# lines 250-261 are doing algorithmically.
setwd("~/Dropbox/oxss_21")
emw <- read_dta("../data/efmw_replication.dta")
emw <- read_dta("data/efmw_replication.dta")
